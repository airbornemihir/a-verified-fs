
%%%%%%%%%%%%%%%%%%%%%%% file typeinst.tex %%%%%%%%%%%%%%%%%%%%%%%%%
%
% This is the LaTeX source for the instructions to authors using
% the LaTeX document class 'llncs.cls' for contributions to
% the Lecture Notes in Computer Sciences series.
% http://www.springer.com/lncs       Springer Heidelberg 2006/05/04
%
% It may be used as a template for your own input - copy it
% to a new file with a new name and use it as the basis
% for your article.
%
% NB: the document class 'llncs' has its own and detailed documentation, see
% ftp://ftp.springer.de/data/pubftp/pub/tex/latex/llncs/latex2e/llncsdoc.pdf
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\documentclass[runningheads,a4paper]{llncs}

\usepackage{amssymb}
\setcounter{tocdepth}{3}
\usepackage{graphicx}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{matrix}

\usepackage{url}
\urldef{\mailsa}\path|mihir@cs.utexas.edu|
%% \urldef{\mailsa}\path|{mihir,|
%% \urldef{\mailsc}\path|hunt}@cs.utexas.edu|
\newcommand{\keywords}[1]{\par\addvspace\baselineskip
\noindent\keywordname\enspace\ignorespaces#1}

\begin{document}

\mainmatter  % start of an individual contribution

% first the title is needed
\title{Formalising filesystems in the ACL2 theorem prover:\\ an
  application to a FAT32-like filesystem}

% a short form should be given in case it is too long for the running head
\titlerunning{ACL2 filesystem verification}

% the name(s) of the author(s) follow(s) next
%
% NB: Chinese authors should write their first names(s) in front of
% their surnames. This ensures that the names appear correctly in
% the running heads and the author index.
%
\author{Mihir Parang Mehta}
%
\authorrunning{Lecture Notes in Computer Science: Authors' Instructions}
% (feature abused for this document to repeat the title also on left hand pages)

% the affiliations are given next; don't give your e-mail address
% unless you accept that it will be published
\institute{University of Texas at Austin, Department of Computer Science,\\
2317 Speedway, Austin, TX 78712, USA\\}

%
% NB: a more complex sample for affiliations and the mapping to the
% corresponding authors can be found in the file "llncs.dem"
% (search for the string "\mainmatter" where a contribution starts).
% "llncs.dem" accompanies the document class "llncs.cls".
%

\toctitle{ACL2 filesystem verification}
\tocauthor{Mihir Mehta}
\maketitle

\begin{abstract}
  We describe an effort to formally verify the FAT32 filesystem, based
  on a specification put together from Microsoft's published
  specification and the Linux kernel source code. We detail our
  approach of proving properties through refinement of filesystem
  models. We describe how this work is applicable to more filesystems
  than solely FAT32, and enumerate possible future applications of
  these techniques.
\keywords{interactive theorem proving, filesystems}
\end{abstract}

\section{Introduction and overview}

Filesystems are ubiquitous in computing, providing application
programs a means to store data persistently, address data by a name
instead of a numeric index, and communicate with other programs.
Thus, the vast majority of application programs
directly or indirectly rely upon filesystems, making filesystem
correctness bugs into serious issues, and making filesystem
verification a critically important issue. Here, we present a
formalisation effort in ACL2 for a filesystem with a FAT32-like data
organisation, and a proof of the read-over-write properties for this
filesystem. By starting with a high-level abstract model and refining
\cite{abadi1991existence} it with successive models which add more of
the complexity of the real filesystem, we are able to manage the
complexity of this proof, which has not yet been attempted. Thus, this
paper contributes a case study in refinement for filesystem
verification, and progress towards the ultimate goal of a
binary-compatible model of a FAT32, which is a real and widely-used
filesystem.

In the rest of this paper, we describe these
models and the properties proved with examples; we proceed to a
high-level explanation of our refinement proofs; and further we offer
some insights about the low-level issues encountered while working the
proofs. We end with some statistics pertaining to the magnitude of the
proof effort and the running time of the proofs.

\section{Related work}

In the literature, much of the work on verifying filesytems has
followed a pattern of synthesising a new filesystem based on a
specification chosen for its ease in proving properties of interest,
such as crash consistency of a journalling system. While we are trying
to work with the specification of an existing filesystem, namely
FAT32, many of these efforts use theorem proving tools like we
do. Interactive theorem provers offer manual control the proof
process, as opposed to non-interactive theorem provers which are more
automated in their functioning; this is a key differentiator.

\subsection{Interactive theorem provers}
An early effort in the filesystem verification domain was by Bevier
and Cohen~\cite{bevier1996executable}, who specified the Synergy
filesystem and created an executable model of the same in ACL2
\cite{kaufmann_manolios_moore_2000}, down to the level of processes
and file descriptors. On the proof front, they certified their model
to preserve well-formedness of their data structures through their
various file operations; however, they did not attempt to prove, for
instance, read-over-write properties or crash consistency. Later,
Klein et al with the SeL4 project~\cite{klein2009sel4} used
Isabelle/HOL~\cite{nipkow2002isabelle} to verify a microkernel;
while their design abstracted away file operations in order to keep
their trusted computing base small, it did serve as a precursor to their
more recent COGENT project~\cite{amani2016cogent}. Here the authors
built a "verified compiler" of sorts, generating C-language code from
specifications in their domain-specific in a manner guaranteed to
avoid many common filesystem bugs. Elsewhere, the SibylFS
project~\cite{ridge2015sibylfs}, again using Isabelle/HOL, provided
an executable specification for filesystems at a level of abstraction
that could function across multiple operating systems including OSX
and Unix. The Coq prover \cite{bertot2013interactive} has also been
used, for instance, for FSCQ
\cite{DBLP:conf/usenix/ChenZCCKZ16}, a state-of-the art filesystem
which was built to have high performance and formally verified crash
consistency properties.

\subsection{Non-interactive theorem provers}
Non-interactive theorem provers such as Z3 \cite{de2008z3}
have also been used; Hyperkernel
\cite{Nelson:2017:HPV:3132747.3132748} is a recent effort which
focusses on simplifying the xv6 microkernel until the point that Z3
can verify it with its SMT solving techniques. However, towards this
end, all system calls in Hyperkernel are replaced with analogs which
can terminate in constant time; while this approach is theoretically
sound, it increases the chances of discrepancies between the model and
the implementation which may diminish the utility of the proofs or
even render them moot. A stronger effort in the same domain is
Yggdrasil~\cite{sigurbjarnarson2016push}, which focusses on verifying
filesystems with the use of Z3. While the authors make substantial
progress in terms of the number of filesystem calls they support and
the crash consistency guarantees they provide, they are subject to
the limits of SMT solving which prevent them from modelling essential
filesystem features such as extents, which are central to FAT32 among
others.

\section{Refinement}

One traditional approach for verification of complex systems is
axiomatic, wherein the desired properties of a system are enumerated
and then verified. This is in contrast with abstraction refinement,
where a system is proved to refine a simpler system, possibly a state
machine or a pseudocode program, which is known to show the desired
properties either by inspection or by proof. The relative merits of
these approaches have been debated in the literature;
Lamport~\cite{lamport1993verification} makes the argument that the
axiomatic style is hopelessly tedious for any but the simplest
systems.

In the present verification endeavour, we choose to verify
refinement properties in a series of successive models. This is also
the approach chosen by Yggdrasil~\cite{sigurbjarnarson2016push}. We do
choose read-over-write properties as axioms, which we prove true in all
models; however, these proofs are obtained more or less "for free"
once a proof is formulated for the base model. Yet, the value of the
refinement approach is attested to by the ease of verification of
several incidental properties, such as the ability of write operations
to succeed as long as there is sufficient space in a filesystem of
finite size.

\section{The FAT32 filesystem}

Microsoft, in its specification \cite{microsoft_2000} defines three
closely related filesystems, named FAT12, FAT16 and FAT32 based on the
bit-width of entries in their \textit{file allocation table} data
structure. Of these, the former two have passed almost into disuse,
while FAT32 continues to be used in media of small capacity, such as
USB thumb drives.

FAT32, while simple, adds some complexity compared to the filesystems
which came before. Regular files, in storage, are divided into
\textit{clusters} (sometimes called \textit{extents}) of a fixed size,
which is decided at the time a FAT32 volume is formatted, and
constrained to be a multiple of the disk sector size. Directory files
are treated much the same way, with the
addition of a file attribute that indicates the file is a
directory. This attribute indicates that the contents of the directory
are a series of 32-bit wide directory entries, one for each file,
containing information including file name, file size, first cluster
index, and access times.

The file allocation table itself contains, very simply, a number of
linked lists. It maps each cluster index used by a file to either the
next cluster index for that file or an end-of-file value defined by the
specification. This allows the contents of a file to be reconstructed
by reading just the first cluster index from its directory entry, and
reconstructing the list of clusters using the table. Unused clusters
are mapped to 0 in the table; this fact is used for counting and
allocating free clusters.

A diagram on the next page illustrates the file allocation table and
data layout for a small directory tree.

\begin{figure}
  \includegraphics[page=1,width=1.10\textwidth]{fat32-diagram.pdf}
\end{figure}

\section{The models}

For every read or write operation, FAT32 requires one or more lookups
into the file allocation table, followed by the corresponding lookups
into the data region. This makes proof efforts about these operations
complex, which serves as the motivation for modelling the filesystem
in a series of steps.

\begin{table}[]
  \centering
  \caption{Models and their features}
  \label{model-description-table}
  \begin{tabular}{|l|p{120mm}|}
    \hline
    \texttt{L1} & The filesystem is represented as a tree, with leaf
    nodes for regular files and non-leaf nodes for
    directories. The contents of regular files are represented as
    strings stored in the nodes of the tree; the storage available for
    these is unbounded. \\ \hline
    \texttt{L2} & A single element of metadata, \textit{length}, is
    stored within each regular file.  \\ \hline
    \texttt{L3} & The contents of regular files are divided into
    blocks of fixed size. These blocks are stored in an external
    "disk" data structure; the storage for these blocks remains
    unbounded. \\ \hline
    \texttt{L4} & The storage available for blocks is now bounded. An
    allocation vector data structure is introduced to help allocate
    and garbage collect blocks. \\ \hline
    \texttt{L5} & Additional metadata for file ownership and access
    permissions is stored within each regular file. \\ \hline
    \texttt{L6} & The allocation vector is replaced by a file
    allocation table, per the official FAT specification. \\ \hline
  \end{tabular}
\end{table}

\begin{figure}
  \centering
  \caption{Refinement relationships between models}
  \label{refinement-figure}
  \begin{tikzpicture}[sibling distance=10em,
      every node/.style = {shape=rectangle, rounded corners,
        draw, align=center,
        top color=white, bottom color=blue!20}]
    \node {L1 - tree}
    child { node {L2 - length}
      child { node {L3 - disk}}
      child { node {L4 - garbage collection}
        child { node {L5 - permissions}}
        child { node {L6 - file allocation table}}}};
  \end{tikzpicture}
\end{figure}

At this point in development, we have six models of the filesystem,
here referred to as \texttt{L1} through \texttt{L6} (see
table \ref{model-description-table}). Each new model
\textit{refines} a previous model, adding some features and
complexity, and thereby approaching closer to a model which is binary
compatible with FAT32. These refinement relationships are shown in
figure \ref{refinement-figure}. \texttt{L1} is the simplest of these,
representing the filesystem as a literal directory tree; later models feature
file metadata (including ownership and permissions), externalisation
of file contents, and allocation/file allocation using an allocation
vector after the fashion of the CP/M file system.

Broadly, we characterise the filesystem
operations we offer as either \textit{write} operations, which do
modify the filesystem, or \textit{read} operations, which do not. In
each model, we have been able to prove \textit{read-over-write}
properties which show that write operations have
their effects made available immediately for reads at the same
location, but also that they do not affect reads at other locations.

The first read-after-write theorem states that immediately following a
write of some text at some location, a read of the same length at the
same location yields the same text. The second read-after-write
theorem states that after a write of some text at some location, a
read at any other location returns exactly what it would have returned
before the write. As an example, listings for the \texttt{L1} versions
of these theorems follow.

\medskip

\noindent
\begin{verbatim}
(defthm l1-read-after-write-1
  (implies (and (l1-fs-p fs)
                (stringp text)
                (symbol-listp hns)
                (natp start)
                (equal n (length text))
                (stringp (l1-stat hns fs)))
           (equal (l1-rdchs hns (l1-wrchs hns fs start text) start n) text)))

(defthm l1-read-after-write-2
  (implies (and (l1-fs-p fs)
                (stringp text2)
                (symbol-listp hns1)
                (symbol-listp hns2)
                (not (equal hns1 hns2))
                (natp start1)
                (natp start2)
                (natp n1)
                (stringp (l1-stat hns1 fs)))
           (equal (l1-rdchs hns1 (l1-wrchs hns2 fs start2 text2) start1 n1)
                  (l1-rdchs hns1 fs start1 n1))))
\end{verbatim}

By composing these properties, we can reason about executions
involving multiple reads and writes, as shown in the following
throwaway lemma.

\medskip

\noindent
\begin{verbatim}
(thm
 (implies (and (l1-fs-p fs)
               (stringp text1)
               (stringp text2)
               (symbol-listp hns1)
               (symbol-listp hns2)
               (not (equal hns1 hns2))
               (natp start1)
               (natp start2)
               (stringp (l1-stat hns1 fs))
               (equal n1 (length text1)))
          (equal (l1-rdchs hns1
                           (l1-wrchs hns2 (l1-wrchs hns1 fs start1 text1)
                                     start2 text2)
                           start1 n1)
                 (l1-rdchs hns1 (l1-wrchs hns1 fs start1 text1)
                           start1 n1))))
\end{verbatim}

\section{Proof methodology}

In \textit{l1}, our simplest model, the read-over-write properties
were, of necessity, proven from scratch.

In each subsequent model, the read-over-write properties are proven as
corollaries of equivalence proofs which establish the correctness of
read and write operations in the respective model with respect to a
previous model. A representation of such an equivalence proof can be
seen in figures \ref{l2-wrchs-correctness-1},
\ref{l2-rdchs-correctness-1} and \ref{l2-read-over-write-1}, which
respectively show the equivalence proof for \texttt{l2-wrchs}, the
equivalence proof for \texttt{l2-rdchs} and the composition of these
to obtain the first read-over-write theorem for model \texttt{L2}.


\begin{figure}
  \centering
  \caption{l2-wrchs-correctness-1}
  \label{l2-wrchs-correctness-1}
  \begin{tikzpicture}
    \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
            {
              l2 & l2 \\
              l1 & l1 \\};
            \path[-stealth]
            (m-1-1) edge node [left] {l2-to-l1-fs} (m-2-1)
            edge node [below] {write} (m-1-2)
            (m-2-1.east|-m-2-2) edge node [below] {write} (m-2-2)
            (m-1-2) edge node [left] {l2-to-l1-fs} (m-2-2);
  \end{tikzpicture}
\end{figure}

\begin{figure}
  \centering
  \caption{l2-rdchs-correctness-1}
  \label{l2-rdchs-correctness-1}
  \begin{tikzpicture}
    \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
            {
              l2 & text \\
              l1 \\};
            \path[-stealth]
            (m-1-1) edge node [left] {l2-to-l1-fs} (m-2-1)
            edge node [below] {read} (m-1-2)
            (m-2-1.east|-m-2-2) edge node [below] {read} (m-1-2);
  \end{tikzpicture}
\end{figure}

\begin{figure}
  \centering
  \caption{l2-read-over-write-1}
  \label{l2-read-over-write-1}
  \begin{tikzpicture}
    \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
            {
              l2 & l2 & text \\
              l1 & l1 \\};
            \path[-stealth]
            (m-1-1) edge node [left] {l2-to-l1-fs} (m-2-1)
            edge node [below] {write} (m-1-2)
            (m-2-1.east|-m-2-2) edge node [below] {write} (m-2-2)
            (m-1-2) edge node [left] {l2-to-l1-fs} (m-2-2)
            edge node [below] {read} (m-1-3)
            (m-2-2.east) edge node [below] {read} (m-1-3);
  \end{tikzpicture}
\end{figure}

\section{Some proof details}

\subsection{Invariants}

As the models grow more complex, with the addition of more auxiliary
data the "sanity" criteria for filesystem instances become more
complex. For instance, in \texttt{L4}, the predicate \texttt{l4-fs-p}
is defined to be the same as \texttt{l3-fs-p}, which recursively
defines the shape of a valid directory tree. However, we choose to
require two more properties for a "sane" filesystem.

\begin{enumerate}
\item Each disk index assigned to a regular file should be
  marked as \textit{used} in the allocation vector - this is essential
  to prevent filesystem errors.
\item Each disk index assigned to a regular file should be distinct
  from all other disk indices assigned to files - this does not hold
  true, for example, in filesystems with hardlinks, but makes our
  proofs easier.
\end{enumerate}

These properties are invariants to be maintained across
write operations; they simplify the verification of read-after-write
properties by ensuring that write properties do not create an
"aliasing" situation in which a regular file's contents can be
modified through a write to a different regular file.

These properties, in the form of the predicates
\texttt{indices-marked-listp} and \texttt{no-duplicatesp}, are
packaged together into the \texttt{l4-stricter-fs-p} predicate, for
which a listing follows.

\medskip

\noindent
\begin{verbatim}
(defun l4-stricter-fs-p (fs alv)
  (declare (xargs :guard t))
  (and (l4-fs-p fs)
       (boolean-listp alv)
       (let ((all-indices (l4-list-all-indices fs)))
            (and (no-duplicatesp all-indices)
                 (indices-marked-p all-indices alv)))))
\end{verbatim}

\subsection{Refinement}

As noted earlier, using a refinement methodology allows us to derive
our read-over-write properties essentially "for free"; more precisely,
we are able to prove read-over-write properties simply with
\texttt{:use} hints after having done the work of proving refinement
through induction.

At a lower level, we are also able to benefit from a happy coincidence
where the CP/M filesystem's allocation vector is an abstraction of
FAT32's file allocation's table. Having proved this refinement
relationship, it becomes a lot easier to prove that \texttt{L4}, which
uses an allocation vector, is an abstraction of \texttt{L6}, which
uses a file allocation table, and this means a lot of effort spent on
proving the invariants described above for \texttt{L4} need not be
replicated for \texttt{L6}.

\subsection{Performance hacking}

As in all ACL2 verification efforts, our work accumulated a number of
helper functions and lemmata in the service of the big-picture proofs,
and these were prone to slow down our proofs somewhat. Thus, using
ACL2's \texttt{accumulated-persistence} tool, we made an effort to
trim the number of enabled rules by focussing on the rules which the
tool suggested to be \textit{useless}. This was important in helping us reduce
the certification time for \texttt{L6} from 229 seconds to 84 seconds,
but from this point onwards results were mixed. As an illustrative
example, disabling the function \texttt{l6-wrchs} brought down the
certification time for \textt{l6} from 84 seconds to 60 seconds, yet
disabling another function, \texttt{l4-collect-all-index-lists}, had a
negligible effect on other books and actually served to increase the
certification time from 60 seconds to 69 seconds. Needless to say, the
latter change was rolled back; a pertinent explanation can be found in
the ACL2 documentation topic \texttt{accumulated-persistence-subtleties}.

%%  It is interesting to note that disabling
%%  l4-collect-all-index-lists brings down the certification time for l4
%%  from 19 seconds to 18 seconds, the certification time for l5 from
%%  21 seconds to 22 seconds and the
%% that for l6 from 60 seconds to 69 seconds.
%% It is interesting to note that disabling l6-wrchs brings down the
%% certification time for \texttt{L6} from 84 seconds to 60 seconds.

\section{Evaluation}
At present, the codebase spans 11710 lines of ACL2 code, including 152
function definitions and 616 theorems and lemmas. Some of this data was
obtained by David A. Wheeler's sloccount tool.

In table \ref{certification-timing-table} we note the time taken to certify
the models in ACL2, as well as some infrastructure upon which the
models are built.

\begin{table}[]
  \centering
  \caption{Time taken to prove models}
  \label{certification-timing-table}
  \begin{tabular}{ll}
    \texttt{L1} & 1s \\
    \texttt{L2} & 5s \\
    \texttt{L3} & 6s \\
    \texttt{L4} & 19s \\
    \texttt{L5} & 21s \\
    \texttt{L6} & 60s \\
    Misc. & 4s \\
  \end{tabular}
\end{table}

\section{Future work}

We are pursuing future work in a few different directions. Primarily,
our next goal is to dispense with the tree representation and
implement filesystem traversal by looking up entries in directory
files. This will yield a model which is entirely contained in a disk
data structure and which can further be validated by co-simulation
with a FAT32 implementation, such as the one shipped with the Linux
kernel.

Next, we hope to re-use some artefacts of verifying FAT32 in order to
verify a more complex filesystem, such as ext4. Choosing a filesystem
with journalling will allow us to model crash consistency.

Finally, we hope to support "code proofs", by providing a basis for
reasoning about filesystem operations in filesystem-specific utilities
such as \texttt{fsck}, as well as other application programs. This is
a large part of the motivation for pursuing binary compatibility.

\subsubsection*{Acknowledgments.} This work has been supported by a
grant from the NSF.

\bibliographystyle{splncs}
\bibliography{references}

\end{document}
