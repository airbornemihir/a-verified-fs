\documentclass[submission,copyright,creativecommons]{eptcs}
\providecommand{\event}{ACL2 workshop 2018} % Name of the event you are submitting to

%% Todo list
%% Gotta substantiate the co-simulations we talked about...
%% Gotta talk about the number of things we re-used from L6 in M2
%% Gotta de-emphasise the proofs for L1 through L6 - they're OK but
%%  not that important. However, the read-over-write proofs for M1
%%  need to be finished.
%% Gotta substantiate the point about code re-use in CP/M and FAT32 -
%%  obviously there isn't enough time to do anything about ext4
%% Gotta talk about concurrent calls in future work - and update that
%%  whole section, obviously
%% Gotta substantiate the claim about non-determinism in M1
%% Gotta fix the darned diagram! (done)

\usepackage{amssymb}
\usepackage{graphicx}
\usepackage{tikz}
\usetikzlibrary{matrix}

\usepackage{breakurl}             % Not needed if you use pdflatex only.
\usepackage{underscore}           % Only needed if you use pdflatex.

\title{Formalising filesystems in the ACL2 theorem prover:\\ an
  application to FAT32}
\author{Mihir Parang Mehta
\institute{Department of Computer Science\\
University of Texas at Austin\\
Austin, TX, USA}
\email{mihir@cs.utexas.edu}}
\def\titlerunning{FAT32 verification}
\def\authorrunning{M.P. Mehta}
\begin{document}
\maketitle

\begin{abstract}
In this work, we present an
approach towards constructing executable specifications of existing
filesystems and verifying their functional properties in a theorem
proving environment. We detail an application of this approach to the
FAT32 filesystem.

We also detail the methodology used to build up this type of
executable specification through a series of models which
incrementally add features of the target filesystem. This methodology
has the benefit of allowing the verification effort to start from
simple models which encapsulate features common to many filesystems
and which are thus suitable for re-use.
\end{abstract}

\section{Introduction and overview}

Filesystems are ubiquitous in computing, providing application
programs a means to store data persistently, address data by a name
instead of a numeric index, and communicate with other programs.
Thus, the vast majority of application programs
directly or indirectly rely upon filesystems, which makes filesystem
verification critically important. Here, we present a
formalisation effort in ACL2 for an implementation of the FAT32
filesystem, and a proof of the read-over-write properties for this
filesystem. By starting with a high-level abstract model and adding
more filesystem features in successive models, we are able to manage the
complexity of this proof, which has not, to our knowledge, been
previously attempted. Thus, this paper contributes an implementation
of FAT32, a widely-used filesystem, formally verified against an
abstract specification and tested for binary compatibility by means of
co-simulation.

In the rest of this paper, we describe these filesystem
models and the properties proved with examples; we proceed to a
high-level explanation of these proofs and the co-simulation
infrastructure; and further we offer some insights about the low-level
issues encountered while working the proofs.
%% I'm not sure I want to keep the statistics. Certainly ACL2 folks
%% don't want to see them.
%% We end with some statistics pertaining to the magnitude of the
%% proof effort and the running time of the proofs.

\section{Related work}

Filesystem verification research has largely followed a pattern of
synthesising a new filesystem based on a specification chosen for its
ease in proving properties of interest, rather than similarity to an
existing filesystem. Our work, in contrast, follows the FAT32
specification closely. In spirit, our work is closer to previous work
which uses interactive theorem provers and explores deep functional
properties than to efforts which use non-interactive theorem provers
such as Z3 to produce fully automated proofs of simpler properties.

\subsection{Interactive theorem provers}
An early effort in the filesystem verification domain was by Bevier
and Cohen~\cite{bevier1996executable}, who specified the Synergy
filesystem and created an executable model of the same in ACL2
\cite{kaufmann2000}, down to the level of processes
and file descriptors. On the proof front, they certified their model
to preserve well-formedness of their data structures through their
various file operations; however, they did not attempt to prove, for
instance, read-over-write properties or crash consistency. Later,
Klein et al with the SeL4 project~\cite{klein2009sel4} used
Isabelle/HOL~\cite{nipkow2002isabelle} to verify a microkernel;
while their design abstracted away file operations in order to keep
their trusted computing base small, it did serve as a precursor to their
more recent COGENT project~\cite{amani2016cogent}. Here the authors
built a "verified compiler" of sorts, generating C-language code from
specifications in their domain-specific in a manner guaranteed to
avoid many common filesystem bugs. Elsewhere, the SibylFS
project~\cite{ridge2015sibylfs}, again using Isabelle/HOL, provided
an executable specification for filesystems at a level of abstraction
that could function across multiple operating systems including OSX
and Unix. The Coq prover \cite{bertot2013interactive} has also been
used, for instance, for FSCQ
\cite{DBLP:conf/usenix/ChenZCCKZ16}, a state-of-the art filesystem
which was built to have high performance and formally verified crash
consistency properties.

\subsection{Non-interactive theorem provers}
Non-interactive theorem provers such as Z3 \cite{de2008z3}
have also been used; Hyperkernel
\cite{Nelson:2017:HPV:3132747.3132748} is a recent effort which
focusses on simplifying the xv6 microkernel until the point that Z3
can verify it with its SMT solving techniques. However, towards this
end, all system calls in Hyperkernel are replaced with analogs which
can terminate in constant time; while this approach is theoretically
sound, it increases the chances of discrepancies between the model and
the implementation which may diminish the utility of the proofs or
even render them moot. A stronger effort in the same domain is
Yggdrasil~\cite{sigurbjarnarson2016push}, which focusses on verifying
filesystems with the use of Z3. While the authors make substantial
progress in terms of the number of filesystem calls they support and
the crash consistency guarantees they provide, they are subject to
the limits of SMT solving which prevent them from modelling essential
filesystem features such as extents, which are central to many
filesystems including FAT32.

\section{Program architecture and performance considerations}

We number our abstract filesystem models \texttt{L1} through
\texttt{L6}, and our concrete models \texttt{M1} through \texttt{M2}
(concrete). These models are constructed incrementally to allow for
reuse of features in general, and a refinement relation where
possible.

Starting with \texttt{L1}, an abstract model representing the directory
structure as a tree, we add file
metadata in \texttt{L2}. We branch off from \texttt{L2} into
\texttt{L3}, a model with file contents broken up into blocks and
stored in an external disk-like data structure, and \texttt{L4}, a
model which also breaks file contents up into disk blocks,
additionally with garbage collection through reference counting, and a
fixed disk size bounding the total size of file contents. To implement
the reference counter in \texttt{L4}, we use an allocation vector as
in the CP/M filesystem, which happens to be the previous technology
target of this work prior to FAT32. We are able to refine \texttt{L4}
into \texttt{L6}, a filesystem with exactly the same properties but
additionally featuring a FAT32-like file allocation table for
allocation and garbage collection, since this data structure happens
to refine the allocation vector.

\texttt{M1} is another tree model, which hews somewhat closely to the
FAT32 specification; it is also used as the in-memory format for
\texttt{M2}, a real model which reads and writes FAT32 disk
images. The operations currently supported are \texttt{pread(2)},
\texttt{pwrite(2)}, and \texttt{stat(2)}. With these operations, we
are able to model the file operations used in standard disk utilities
such as cat(1) and dd(1), with a view to writing formal specifications
of these programs later.

There is not a refinement relation between the abstract models and the
concrete models, but we re-use many of our theorems from \texttt{L6}
in \texttt{M2} since the file allocation data structure in \texttt{L6}
follows the FAT32 specification.

In order to obtain reasonable execution performance, we implement
\texttt{M2} as a single-threaded object. Most importantly, this allows
us to model the file allocation table and data region, which are long
arrays, as arrays in Common Lisp rather than as lists, which have poor
performance for update operations on their elements because of the
number of cons cells which must be created and garbage collected for
each update. Array operations in single threaded objects are subject
to certain syntactic restrictions to prevent copies of arrays from
being created as usually happens with Lisp objects; to simplify the
task of reading and writing disk images under these restrictions, we
also create a library of useful macros.

We choose to implement a subset of the POSIX filesystem application
programming interface. This allows us to easily compare the results of
running filesystem operations on \texttt{M2} and the Linux kernel's
implementation of FAT32, which in turn allows us to test our
implementation's correctness through co-simulation in addition to
theorem proving. One trade-off for this choice is the necessity of
emulating certain functionality provided by the Linux kernel to all
filesystems; thus, we implement process tables and file tables through
a straightforward approach similar to that used in
Synergy~\cite{bevier1996executable}.

\section{The FAT32 filesystem}

FAT32 was initially developed at Microsoft~\cite{microsoft2000} in
order to address certain shortcomings of the DOS filesystem previously
in use in their operating systems. While it is simple by today's
standards, it does add some complexity compared to the filesystems
which came before.

All files, including regular files and directory files, are divided into
\textit{clusters} (sometimes called \textit{extents}) of a fixed size,
which is decided at the time a FAT32 volume is formatted, and
constrained to be a multiple of the disk sector size. Directory files
differ only in a metadata attribute which indicates that their contents
should be treated as a sequence of directory entries. Each such
directory entry is 32 bytes wide and contains information including
name, size, first cluster index, and access times for the
corresponding file.

The file allocation table itself contains a number of linked lists. It
maps each cluster index used by a file to either the next cluster
index for that file or a special end-of-clusterchain value \footnote{
There is actually a range of end-of-clusterchain values in the
specification, not just one. We support all values in the range.} This
allows the contents of a file to be reconstructed by
reading just the first cluster index from the corresponding directory
entry, and building the list of clusters using the table. Unused
clusters are mapped to 0 in the table; this fact is used for counting
and allocating free clusters.

We illustrate the file allocation table and data layout for a small
directory tree in figure~\ref{fat32-example}.

\begin{figure}
  \centering
  \caption{A FAT32 directory tree}
  \label{fat32-example}
  \begin{tikzpicture}[sibling distance=5em,
      every node/.style = {shape=rectangle, rounded corners,
        draw, align=center,
        top color=white, bottom color=violet!20}]
    \node {/}
    child { node {vmlinuz}}
    child { node {initrd.img}}
    child { node {tmp/}
      child { node {ticket1.txt}}
      child { node {ticket2.txt}}};
  \end{tikzpicture}

  \begin{tabular}{|c|c|}
    \hline
    FAT index & FAT entry \\ \hline
    0 &  \\ \hline
    1 &  \\ \hline
    2 & \textit{EOC} \\ \hline
    3 & 4 \\ \hline
    4 & \textit{EOC} \\ \hline
    5 & \textit{EOC} \\ \hline
    6 & \textit{EOC} \\ \hline
    7 & \textit{EOC} \\ \hline
    8 & \textit{EOC} \\ \hline
    9 & 0 \\ \hline
    \vdots & \vdots
  \end{tabular}\\

  \begin{tabular}{|c|c|}
    \hline
       & Directory entry in / \\ \hline
    0  & "vmlinuz", 3 \\ \hline
    32 & "initrd.img", 5 \\ \hline
    64 & "tmp", 6 \\ \hline
    \vdots & \vdots
  \end{tabular}

  \begin{tabular}{|c|c|}
    \hline
       & Directory entry in /tmp/ \\ \hline
    0  & "ticket1", 7 \\ \hline
    32 & "ticket2", 8 \\ \hline
    \vdots & \vdots
  \end{tabular}
\end{figure}

%% \begin{figure}
%%   \includegraphics[page=1,width=1.10\textwidth]{fat32-diagram.pdf}
%% \end{figure}

\section{The models}

For every read or write operation, FAT32 requires one or more lookups
into the file allocation table, followed by the corresponding lookups
into the data region. This makes proof efforts about these operations
complex, which serves as the motivation for modelling the filesystem
in a series of steps.

\begin{table}[]
  \centering
  \caption{Abstract models and their features}
  \label{abstract-model-description-table}
  \begin{tabular}{|l|p{120mm}|}
    \hline
    \texttt{L1} & The filesystem is represented as a tree, with leaf
    nodes for regular files and non-leaf nodes for
    directories. The contents of regular files are represented as
    strings stored in the nodes of the tree; the storage available for
    these is unbounded. \\ \hline
    \texttt{L2} & A single element of metadata, \textit{length}, is
    stored within each regular file.  \\ \hline
    \texttt{L3} & The contents of regular files are divided into
    blocks of fixed size. These blocks are stored in an external
    "disk" data structure; the storage for these blocks remains
    unbounded. \\ \hline
    \texttt{L4} & The storage available for blocks is now bounded. An
    allocation vector data structure is introduced to help allocate
    and garbage collect blocks. \\ \hline
    \texttt{L5} & Additional metadata for file ownership and access
    permissions is stored within each regular file. \\ \hline
    \texttt{L6} & The allocation vector is replaced by a file
    allocation table, matching the official FAT specification. \\ \hline
  \end{tabular}
\end{table}

\begin{table}[]
  \centering
  \caption{Concrete models}
  \label{concrete-model-description-table}
  \begin{tabular}{|l|p{120mm}|}
    \hline
    \texttt{M1} & The filesystem is represented as a tree, as in
    \texttt{L1}. Write operations show non-determinism in order to
    model disk capacity errors in a real filesystem. \\ \hline
    \texttt{M2} & A single element of metadata, \textit{length}, is
    stored within each regular file.  \\ \hline
  \end{tabular}
\end{table}

\begin{figure}
  \centering
  \caption{Refinement relationships between models}
  \label{refinement-figure}
  \begin{tikzpicture}[sibling distance=15em,
      every node/.style = {shape=rectangle, rounded corners,
        draw, align=center,
        top color=white, bottom color=blue!20}]
    \node {L1 - tree}
    child { node {L2 - length}
      child { node {L3 - unbounded disk}}
      child { node {L4 - bounded disk with garbage collection}
        child { node {L5 - permissions}}
        child { node {L6 - file allocation table}}}};
  \end{tikzpicture}
\end{figure}

At this point in development, we have six models of the filesystem,
here referred to as \texttt{L1} through \texttt{L6}, described in
table \ref{abstract-model-description-table}. Each model other than
\texttt{L1}
refines a previous model, adding some features and complexity and
thereby approaching closer to a model which is binary compatible with
FAT32. These refinement relationships are shown in figure
\ref{refinement-figure}. \texttt{L1} is the simplest of these,
representing the filesystem as a literal directory tree; later models
feature file metadata (including ownership and permissions),
externalisation of file contents, and allocation/file allocation using
an allocation vector after the fashion of the CP/M file system (this
is a remnant of an earlier filesystem verification effort for CP/M,
which we subsumed into the present work).

Broadly, we characterise the filesystem
operations we offer as either \textit{write} operations, which do
modify the filesystem, or \textit{read} operations, which do not. In
each model, we have been able to prove \textit{read-over-write}
properties which show that write operations have
their effects made available immediately for reads at the same
location, but also that they do not affect reads at other locations.

The first read-after-write theorem states that immediately following a
write of some text at some location, a read of the same length at the
same location yields the same text. The second read-after-write
theorem states that after a write of some text at some location, a
read at any other location returns exactly what it would have returned
before the write. As an example, listings for the \texttt{L1} versions
of these theorems follow.

\medskip

\noindent
\begin{verbatim}
(defthm l1-read-after-write-1
  (implies (and (l1-fs-p fs)
                (stringp text)
                (symbol-listp hns)
                (natp start)
                (equal n (length text))
                (stringp (l1-stat hns fs)))
           (equal (l1-rdchs hns (l1-wrchs hns fs start text) start n) text)))

(defthm l1-read-after-write-2
  (implies (and (l1-fs-p fs)
                (stringp text2)
                (symbol-listp hns1)
                (symbol-listp hns2)
                (not (equal hns1 hns2))
                (natp start1)
                (natp start2)
                (natp n1)
                (stringp (l1-stat hns1 fs)))
           (equal (l1-rdchs hns1 (l1-wrchs hns2 fs start2 text2) start1 n1)
                  (l1-rdchs hns1 fs start1 n1))))
\end{verbatim}

By composing these properties, we can reason about executions
involving multiple reads and writes, as illustrated in the following
throwaway proof.

\medskip

\noindent
\begin{verbatim}
(thm
 (implies (and (l1-fs-p fs)
               (stringp text1)
               (stringp text2)
               (symbol-listp hns1)
               (symbol-listp hns2)
               (not (equal hns1 hns2))
               (natp start1)
               (natp start2)
               (stringp (l1-stat hns1 fs))
               (equal n1 (length text1)))
          (equal (l1-rdchs hns1
                           (l1-wrchs hns2 (l1-wrchs hns1 fs start1 text1)
                                     start2 text2)
                           start1 n1)
                 (l1-rdchs hns1 (l1-wrchs hns1 fs start1 text1)
                           start1 n1))))
\end{verbatim}

\section{Proof methodology}

In \texttt{L1}, our simplest model, the read-over-write properties
were, of necessity, proven from scratch.

In each subsequent model, the read-over-write properties are proven as
corollaries of equivalence proofs which establish the correctness of
read and write operations in the respective model with respect to a
previous model. A representation of such an equivalence proof can be
seen in figures \ref{l2-wrchs-correctness-1},
\ref{l2-rdchs-correctness-1} and \ref{l2-read-over-write-1}, which
respectively show the equivalence proof for \texttt{l2-wrchs}, the
equivalence proof for \texttt{l2-rdchs} and the composition of these
to obtain the first read-over-write theorem for model \texttt{L2}.


\begin{figure}
  \centering
  \caption{l2-wrchs-correctness-1}
  \label{l2-wrchs-correctness-1}
  \begin{tikzpicture}
    \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
            {
              l2 & l2 \\
              l1 & l1 \\};
            \path[-stealth]
            (m-1-1) edge node [left] {l2-to-l1-fs} (m-2-1)
            edge node [below] {write} (m-1-2)
            (m-2-1.east|-m-2-2) edge node [below] {write} (m-2-2)
            (m-1-2) edge node [left] {l2-to-l1-fs} (m-2-2);
  \end{tikzpicture}
\end{figure}

\begin{figure}
  \centering
  \caption{l2-rdchs-correctness-1}
  \label{l2-rdchs-correctness-1}
  \begin{tikzpicture}
    \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
            {
              l2 & text \\
              l1 \\};
            \path[-stealth]
            (m-1-1) edge node [left] {l2-to-l1-fs} (m-2-1)
            edge node [below] {read} (m-1-2)
            (m-2-1.east|-m-2-2) edge node [below] {read} (m-1-2);
  \end{tikzpicture}
\end{figure}

\begin{figure}
  \centering
  \caption{l2-read-over-write-1}
  \label{l2-read-over-write-1}
  \begin{tikzpicture}
    \matrix (m) [matrix of math nodes,row sep=3em,column sep=4em,minimum width=2em]
            {
              l2 & l2 & text \\
              l1 & l1 \\};
            \path[-stealth]
            (m-1-1) edge node [left] {l2-to-l1-fs} (m-2-1)
            edge node [below] {write} (m-1-2)
            (m-2-1.east|-m-2-2) edge node [below] {write} (m-2-2)
            (m-1-2) edge node [left] {l2-to-l1-fs} (m-2-2)
            edge node [below] {read} (m-1-3)
            (m-2-2.east) edge node [below] {read} (m-1-3);
  \end{tikzpicture}
\end{figure}

\section{Some proof details}

We have come to rely on certain principles for the proof effort for
each new model. We summarise these below.

\subsection{Invariants}

As the models grow more complex, with the addition of more auxiliary
data the "sanity" criteria for filesystem instances become more
complex. For instance, in \texttt{L4}, the predicate \texttt{l4-fs-p}
is defined to be the same as \texttt{l3-fs-p}, which recursively
defines the shape of a valid directory tree. However, we choose to
require two more properties for a "sane" filesystem.

\begin{enumerate}
\item Each disk index assigned to a regular file should be
  marked as \textit{used} in the allocation vector - this is essential
  to prevent filesystem errors.
\item Each disk index assigned to a regular file should be distinct
  from all other disk indices assigned to files - this does not hold
  true, for example, in filesystems with hardlinks, but makes our
  proofs easier.
\end{enumerate}

These properties are invariants to be maintained across
write operations; while not all of them are strictly necessary for a
filesystem instance to be valid, they do simplify the verification of
read-after-write properties by helping us ensure that write operations
do not create an "aliasing" situation in which a regular file's
contents can be modified through a write to a different regular file.

These properties, in the form of the predicates
\texttt{indices-marked-listp} and \texttt{no-duplicatesp}, are
packaged together into the \texttt{l4-stricter-fs-p} predicate, for
which a listing follows.

\medskip

\noindent
\begin{verbatim}
(defun l4-stricter-fs-p (fs alv)
  (declare (xargs :guard t))
  (and (l4-fs-p fs)
       (boolean-listp alv)
       (let ((all-indices (l4-list-all-indices fs)))
            (and (no-duplicatesp all-indices)
                 (indices-marked-p all-indices alv)))))
\end{verbatim}

\subsection{Reuse}

As noted earlier, using a refinement methodology allows us to derive
our read-over-write properties essentially "for free"; more precisely,
we are able to prove read-over-write properties simply with
\texttt{:use} hints after having done the work of proving refinement
through induction.

At a lower level, we are also able to benefit from refinement
relationships between components of our different models. For example,
such a relationship exists between the allocation vector used in
\texttt{L4} and the file allocation table used in \texttt{L6}. More
precisely, by taking a file allocation table and mapping each non-zero
entry to \texttt{true} and each zero entry to \texttt{false}, we
obtain a corresponding allocation vector with exactly the same amount
of available space. This is a refinement mapping which makes it a lot
easier to prove that \texttt{L4}, which uses an allocation vector, is
an abstraction of \texttt{L6}, which uses a file allocation
table. This, in turn, means that the effort spent on proving the
invariants described above for \texttt{L4} need not be replicated for
\texttt{L6}.

\section{Co-simulation}

Previous work on executable specifications \cite{} has shown the
importance of testing these on real examples, in order to validate
that the behaviour shown matches that of the system being
specified. In our case, this means we must validate our filesystem by
testing it in execution against a canonical implementation of FAT32;
in this case, we choose the implementation which ships with Linux
kernel 3.10.

We use \texttt{mkfs.fat(8)}, a program which produces FAT32 disk
images, for our tests. When run with the \texttt{-v} flag, this
program emits an English-language summary of the fields of the newly
created disk image; we make use of this by writing an ACL2 program
based on our model which reads the image and reproduces this
summary. This validates our code for reading the various fields of the
disk image and gives us a regression test to use while we modify our
model to support proofs and filesystem calls.

%% This stuff is not yet implemented exactly as described.
Our first co-simulation test is for \texttt{cat(1)}, a simple program
which reads its input and copies it to its output. Its functionality
is reproduced in an ACL2 program which uses our implementations of the
system calls \texttt{lstat(2)}, \texttt{open(2)}, \texttt{pread(2)},
\texttt{pwrite(2)}, and \texttt{close(2)}. This allows us to validate
our code for reading and writing regular files and directories which
span multiple clusters.

%% This stuff is not yet implemented at all.
We further test the \texttt{dd(1)} program, which provides similar
functionality to \texttt{cat} but with more options to customise the
data transfer, for instance, by allowing the data transfer to be split
into blocks of a specified size.

\section{Conclusion}

This work formalises a FAT32-like filesystem and proves
read-over-write properties through refinement of a series of
models. Further, it proves the correctness of FAT32's allocation and
garbage collection mechanisms, and provides artefacts to be used in a
subsequent realistic model of FAT32.

\section{Future work}

Our primary goal is to dispense with the tree representation and
implement filesystem traversal by looking up entries in directory
files. This will also involve addressing a subtle issue where reads
affect the state of a filesystem by means of updating the access time,
which has been analysed earlier in the context of
microprocessors~\cite{goel2017engineering}. This will yield a model
which is entirely contained in a disk data structure, without
auxiliary data structures such as the tree, and which can
further be validated by co-simulation with a FAT32 implementation such
as that of Linux.

Next, we hope to re-use some artefacts of verifying FAT32 in order to
verify a more complex filesystem, such as ext4. Choosing a filesystem
with journalling will allow us to model crash consistency.

Finally, we hope to support "code proofs", by providing a basis for
reasoning about filesystem operations in filesystem-specific utilities
such as \texttt{fsck}, as well as other application programs. This is
a large part of the motivation for pursuing binary compatibility.

\subsubsection*{Acknowledgments.} This material is based upon work
supported by the National Science Foundation SaTC program under
contract number CNS-1525472. Thanks are also due to Warren A. Hunt
Jr. and Matthew J. Kaufmann for their guidance.

\section{Bibliography}

We request that you use
\href{http://eptcs.web.cse.unsw.edu.au/eptcs.bst}
{\tt $\backslash$bibliographystyle$\{$eptcs$\}$}
\cite{bibliographystylewebpage}, or one of its variants
\href{http://eptcs.web.cse.unsw.edu.au/eptcsalpha.bst}{eptcsalpha},
\href{http://eptcs.web.cse.unsw.edu.au/eptcsini.bst}{eptcsini} or
\href{http://eptcs.web.cse.unsw.edu.au/eptcsalphaini.bst}{eptcsalphaini}
\cite{bibliographystylewebpage}. Compared to the original {\LaTeX}
{\tt $\backslash$biblio\-graphystyle$\{$plain$\}$},
it ignores the field {\tt month}, and uses the extra
bibtex fields {\tt eid}, {\tt doi}, {\tt ee} and {\tt url}.
The first is for electronic identifiers (typically the number $n$
indicating the $n^{\rm th}$ paper in an issue) of papers in electronic
journals that do not use page numbers. The other three are to refer,
with life links, to electronic incarnations of the paper.

\paragraph{DOIs}

Almost all publishers use digital object identifiers (DOIs) as a
persistent way to locate electronic publications. Prefixing the DOI of
any paper with {\tt http://dx.doi.org/} yields a URI that resolves to the
current location (URL) of the response page\footnote{Nowadays, papers
  that are published electronically tend
  to have a \emph{response page} that lists the title, authors and
  abstract of the paper, and links to the actual manifestations of
  the paper (e.g.\ as {\tt dvi}- or {\tt pdf}-file). Sometimes
  publishers charge money to access the paper itself, but the response
  page is always freely available.}
of that paper. When the location of the response page changes (for
instance through a merge of publishers), the DOI of the paper remains
the same and (through an update by the publisher) the corresponding
URI will then resolve to the new location. For that reason a reference
ought to contain the DOI of a paper, with a life link to the corresponding
URI, rather than a direct reference or link to the current URL of
publisher's response page. This is the r\^ole of the bibtex field {\tt doi}.
{\bf EPTCS requires the inclusion of a DOI in each cited paper, when available.}

DOIs of papers can often be found through
\url{http://www.crossref.org/guestquery};\footnote{For papers that will appear
  in EPTCS and use \href{http://eptcs.web.cse.unsw.edu.au/eptcs.bst}
  {\tt $\backslash$bibliographystyle$\{$eptcs$\}$} there is no need to
  find DOIs on this website, as EPTCS will look them up for you
  automatically upon submission of a first version of your paper;
  these DOIs can then be incorporated in the final version, together
  with the remaining DOIs that need to found at DBLP or publisher's webpages.}
the second method {\it Search on article title}, only using the {\bf
surname} of the first-listed author, works best.  
Other places to find DOIs are DBLP and the response pages for cited
papers (maintained by their publishers).

\paragraph{The bibtex fields {\tt ee} and {\tt url}}

Often an official publication is only available against payment, but
as a courtesy to readers that do not wish to pay, the authors also
make the paper available free of charge at a repository such as
\url{arXiv.org}. In such a case it is recommended to also refer and
link to the URL of the response page of the paper in such a
repository.  This can be done using the bibtex fields {\tt ee} or {\tt
url}, which are treated as synonyms.  These fields should \textbf{not} be used
to duplicate information that is already provided through the DOI of
the paper.
You can find archival-quality URL's for most recently published papers
in DBLP---they are in the bibtex-field {\tt ee}---but please suppress
repetition of DOI information. In fact, it is often
useful to check your references against DBLP records anyway, or just find
them there in the first place.

\paragraph{Typesetting DOIs and URLs}

When using {\LaTeX} rather than {\tt pdflatex} to typeset your paper, by
default no linebreaking within long URLs is allowed. This leads often
to very ugly output, that moreover is different from the output
generated when using {\tt pdflatex}. This problem is repaired when
invoking \href{http://eptcs.web.cse.unsw.edu.au/breakurl.sty}
{\tt $\backslash$usepackage$\{$breakurl$\}$}: it allows linebreaking
within links and yield the same output as obtained by default with
{\tt pdflatex}. 
When invoking {\tt pdflatex}, the package {\tt breakurl} is ignored.

Please avoid using {\tt $\backslash$usepackage$\{$doi$\}$}, or
{\tt $\backslash$newcommand$\{\backslash$doi$\}$}.
If you really need to redefine the command {\tt doi}
use {\tt $\backslash$providecommand$\{\backslash$doi$\}$}.

The package {\tt $\backslash$usepackage$\{$underscore$\}$} is
recommended to deal with underscores in DOIs. This is not needed when
using {\tt $\backslash$usepackage$\{$breakurl$\}$} and not {\tt pdflatex}.

\paragraph{References to papers in the same EPTCS volume}

To refer to another paper in the same volume as your own contribution,
use a bibtex entry with
\begin{center}
  {\tt series    = $\{\backslash$thisvolume$\{5\}\}$},
\end{center}
where 5 is the submission number of the paper you want to cite.
You may need to contact the author, volume editors or EPTCS staff to
find that submission number; it becomes known (and unchangeable)
as soon as the cited paper is first uploaded at EPTCS\@.
Furthermore, omit the fields {\tt publisher} and {\tt volume}.
Then in your main paper you put something like:

\noindent
{\small \tt $\backslash$providecommand$\{\backslash$thisvolume$\}$[1]$\{$this
  volume of EPTCS, Open Publishing Association$\}$}

\noindent
This acts as a placeholder macro-expansion until EPTCS software adds
something like

\noindent
{\small \tt $\backslash$newcommand$\{\backslash$thisvolume$\}$[1]%
  $\{\{\backslash$eptcs$\}$ 157$\backslash$opa, pp 45--56, doi:\dots$\}$},

\noindent
where the relevant numbers are pulled out of the database at publication time.
Here the newcommand wins from the providecommand, and {\tt \small $\backslash$eptcs}
resp.\ {\tt \small $\backslash$opa} expand to

\noindent
{\small \tt $\backslash$sl Electronic Proceedings in Theoretical Computer Science} \hfill and\\
{\small \tt , Open Publishing Association} \hfill .

\noindent
Hence putting {\small \tt $\backslash$def$\backslash$opa$\{\}$} in
your paper suppresses the addition of a publisher upon expansion of the citation by EPTCS\@.
An optional argument like
\begin{center}
  {\tt series    = $\{\backslash$thisvolume$\{5\}[$EPTCS$]\}$},
\end{center}
overwrites the value of {\tt \small $\backslash$eptcs}.

%% \nocite{*}
\bibliographystyle{eptcs}
\bibliography{references}
\end{document}
